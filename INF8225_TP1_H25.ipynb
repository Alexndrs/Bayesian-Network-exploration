{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAHWMYJv4xuc"
      },
      "source": [
        "# INF8225 TP1 H25 (v2.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qV-6EDgXO-S9"
      },
      "source": [
        "Alexandre - Dréan / 2408681 ########\n",
        "\n",
        "Partie 3 réalisée: [seul(e)]\n",
        "ou avec\n",
        "[Prénom - NOM -\n",
        "Matricule ########]\n",
        "\n",
        "Date limite :\n",
        "\n",
        "20h30 le 6 février 2025 (Partie 1 et 2)\n",
        "\n",
        "20h30 le 20 février 2025 (Partie 3)\n",
        "\n",
        "Remettez votre fichier Colab sur Moodle en 2 formats: **.pdf** ET **.ipynb**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jo2CPniBeytl"
      },
      "source": [
        "**Comment utiliser**:\n",
        "\n",
        "Il faut copier ce notebook dans vos dossiers pour avoir une version que vous pouvez modifier, voici deux façons de le faire:\n",
        "* File / Save a copy in Drive ...\n",
        "* File / Download .ipynb\n",
        "\n",
        "**Pour utiliser un GPU**\n",
        "\n",
        "Runtime / Change Runtime Type / Hardware Accelerator / GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqfqhDoL5CfA"
      },
      "source": [
        "# Partie 2 (20 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjIVlyRq5CjA"
      },
      "source": [
        "## Objectif\n",
        "\n",
        "L’objectif de la partie 2 du travail pratique est de permettre à l’étudiant de se familiariser avec l’apprentissage automatique via la régression logistique. Nous allons donc résoudre un problème de classification d'images en utilisant l’approche de descente du gradient (gradient descent) pour optimiser la log-vraisemblance négative (negative log-likelihood) comme fonction de perte.\n",
        "\n",
        "L'algorithme à implémenter est une variation de descente de gradient qui s’appelle l’algorithme de descente de gradient stochastique par mini-ensemble (mini-batch stochastic gradient descent).  Votre objectif est d’écrire un programme en Python pour optimiser les paramètres d’un modèle étant donné un ensemble de données d’apprentissage, en utilisant un ensemble de validation pour déterminer quand arrêter l'optimisation, et finalement de montrer la performance sur l’ensemble du test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFxYYRQJ5Cnb"
      },
      "source": [
        "## Théorie: la régression logistique et le calcul du gradient\n",
        "\n",
        "\n",
        "Il est possible d’encoder l’information concernant l’étiquetage avec des vecteurs multinomiaux (one-hot vectors), c.-à-d. un vecteur de zéros avec un seul 1 pour indiquer quand la classe $C=k$ dans la dimension $k$. Par exemple, le vecteur $\\mathbf{y}=[0, 1, 0, \\cdots, 0]^T$ représente la deuxième classe. Les caractéristiques (features) sont données par des vecteurs $\\mathbf{x}_i \\in \\mathbb{R}^{D}$. En définissant les paramètres de notre modèle comme : $\\mathbf{W}=[\\mathbf{w}_1, \\cdots, \\mathbf{w}_K]^T$ et $\\mathbf{b}=[b_1, b_2, \\cdots  b_K]^T$ et la fonction softmax comme fonction de sortie, on peut exprimer notre modèle sous la forme :\n",
        "\\begin{eqnarray}\n",
        "    p(\\mathbf{y}|\\mathbf{x})\n",
        "    &=& \\frac{\\exp(\\mathbf{y}^T \\mathbf{W} \\mathbf{x} + \\mathbf{y}^T \\mathbf{b})}{\\sum_{\\mathbf{y}_k \\in \\mathscr{Y}} \\exp(\\mathbf{y}_k^T \\mathbf{W} \\mathbf{x} + \\mathbf{y}_k^T \\mathbf{b})}\n",
        "\\end{eqnarray}\n",
        "L'ensemble de données consiste de $n$ paires (label, input) de la forme $\\mathscr{D}:=(\\mathbf{\\tilde{y}}_i, \\mathbf{\\tilde{x}}_i)_{i=1}^n$, où nous utilisons l'astuce de redéfinir $\\mathbf{\\tilde{x}}_i = [\\mathbf{\\tilde{x}}_i^T 1]^T$ et nous redéfinissions la matrice de paramètres $\\boldsymbol{\\theta} \\in \\mathbb{R}^{K\\times(D+1)}$ (voir des notes de cours pour la relation entre $\\boldsymbol{\\theta}$ et $\\mathbf{W}$). Notre fonction de perte, la log-vraisemblance négative des données selon notre modèle est définie comme:\n",
        "\\begin{equation}\n",
        "    \\mathscr{L}\\big( \\boldsymbol{\\theta}, \\mathscr{D} \\big) := -\\log \\prod_{i=1}^N P(\\mathbf{\\tilde{y}}_i|\\mathbf{\\tilde{x}}_i; \\boldsymbol{\\theta})\n",
        "\\end{equation}\n",
        "Pour cette partie du TP, nous avons calculé pour vous le gradient de la fonction de perte par rapport par rapport aux paramètres du modèle:\n",
        "\\begin{eqnarray}\n",
        "    \\frac{\\partial}{\\partial \\boldsymbol{\\theta}} \\mathscr{L}\\big( \\boldsymbol{\\theta}, \\mathscr{D} \\big)\n",
        "    &=& -\\sum_{i=1}^N \\frac{\\partial}{\\partial \\boldsymbol{\\theta}} \\Bigg\\{\\log \\Bigg(\\frac{\\exp(\\mathbf{\\tilde{y}}_i^T \\boldsymbol{\\theta} \\mathbf{\\tilde{x}}_i)}{\\sum_{\\mathbf{y}_k \\in \\mathscr{Y}} \\exp(\\mathbf{y}_k^T \\boldsymbol{\\theta} \\mathbf{\\tilde{x}}_i)} \\Bigg) \\Bigg\\} \\\\\n",
        "    &=& -\\sum_{i=1}^N \\left(\\mathbf{\\tilde{y}}_i \\mathbf{\\tilde{x}}^T_i- \\sum_{\\mathbf{y}_k \\in \\mathscr{Y}} P(\\mathbf{y}_k|\\mathbf{\\tilde{x}}_i,\\boldsymbol{\\theta}) \\mathbf{y}_k \\mathbf{\\tilde{x}}^T_i \\right) \\\\\n",
        "    &=& \\sum_{i=1}^N \\mathbf{\\hat{p}}_i \\mathbf{\\tilde{x}}^T_i - \\sum_{i=1}^N \\mathbf{\\tilde{y}}_i \\mathbf{\\tilde{x}}^T_i\n",
        "\\end{eqnarray}\n",
        "où $\\mathbf{\\hat{p}}_i$ est un vecteur de probabilités produit par le modèle pour l'exemple $\\mathbf{\\tilde{x}}_i$ et $\\mathbf{\\tilde{y}}_i$ est le vrai *label* pour ce même exemple.\n",
        "\n",
        "Finalement, il reste à discuter de l'évaluation du modèle. Pour la tâche d'intérêt, qui est une instance du problème de classification, il existe plusieurs métriques pour mesurer les performances du modèle la précision de classification, l'erreur de classification, le taux de faux/vrai positifs/négatifs, etc. Habituellement dans le contexte de l'apprentissage automatique, la précision est la plus commune.\n",
        "\n",
        "La précision est définie comme le rapport du nombre d'échantillons bien classés sur le nombre total d'échantillons à classer:\n",
        "$$\n",
        "\\tau_{acc} := \\frac{|\\mathscr{C}|}{|\\mathscr{D}|}\n",
        "$$\n",
        "où l'ensemble des échantillons bien classés $\\mathscr{C}$ est:\n",
        "$$\n",
        "\\mathscr{C} := \\lbrace (\\mathbf{x}, \\mathbf{y}) \\in \\mathscr{D} \\, | \\, \\underset{k}{\\arg\\max} \\, \\, P(\\cdot|\\mathbf{\\tilde{x}}_i; \\boldsymbol{\\theta})_k = \\underset{k}{\\arg\\max} \\, \\, \\tilde{y}_{i,k} \\rbrace\n",
        "$$\n",
        "En mots, il s'agit du sous-ensemble d'échantillons pour lesquels la classe la plus probable selon notre modèle correspond à la vraie classe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ffr5uSLRzkkY"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3wjjnIDGHZj"
      },
      "source": [
        "## Description des tâches\n",
        "\n",
        "#### 1. Code à compléter\n",
        "\n",
        "On vous demande de compléter l'extrait de code ci-dessous pour résoudre ce problème. Vous devez utiliser la librairie PyTorch cette partie du TP: https://pytorch.org/docs/stable/index.html. Mettez à jour les paramètres de votre modèle avec la descente par *mini-batch*. Exécutez des expériences avec trois différents ensembles: un ensemble d’apprentissages avec 90\\% des exemples (choisis au hasard), un ensemble de validation avec 10\\%. Utilisez uniquement l'ensemble de test pour obtenir votre meilleur résultat une fois que vous pensez avoir obtenu votre meilleure stratégie pour entraîner le modèle.\n",
        "\n",
        "#### 2. Rapport à rédiger\n",
        "\n",
        "Présentez vos résultats dans un rapport. Ce rapport devrait inclure:\n",
        "\n",
        "- **Recherche d'hyperparamètres:** Faites une recherche d'hyperparamètres pour différents taux d'apprentissage, e.g. 0.1, 0.01, 0.001, et différentes tailles de mini-batch, e.g. 1, 20, 200, 1000 pour des modèles entrainés avec SGD. Présentez dans un tableau la précision finale du modèle, sur l'*ensemble de validation*, pour ces différentes combinaisons d'hyperparamètres.\n",
        "\n",
        "- **Analyse du meilleur modèle:** Pour votre meilleur modèle, présentez deux figures montrant la progression de son apprentissage sur l'*ensembe d'entrainement et l'ensemble de validation*. La première figure montrant les courbes de log-vraisemblance négative moyenne après chaque epoch, la deuxième montrant la précision du modèle après chaque epoch. Finalement donnez la précision finale sur l'ensemble de test.\n",
        "\n",
        "- **Lire l'article de recherche -\n",
        "Adam**: a method for stochastic optimization. Kingma, D., \\& Ba, J. (2015). International Conference on Learning Representation (ICLR).\n",
        "https://arxiv.org/pdf/1412.6980.pdf. Implémentez Adam, répétez les deux étapes précédentes (recherche d'hyperparamètres et analyse du meilleur modèle) cette fois en utilisat Adam, et comparez les performances finales avec votre meilleur modèle SGD.\n",
        "\n",
        "**IMPORTANT**\n",
        "\n",
        "L'objectif du TP est de vous faire implémenter la rétropropagation à la main. **Il est donc interdit d'utiliser les capacités de construction de modèles ou de différentiation automatique de pytorch -- par exemple, aucun appels à torch.nn, torch.autograd ou à la méthode .backward().** L'objectif est d'implémenter un modèle de classification logistique ainsi que son entainement en utilisant uniquement des opérations matricielles de base fournies par PyTorch e.g. torch.sum(), torch.matmul(), etc."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fonctions fournies"
      ],
      "metadata": {
        "id": "oQq0nDgZuMfs"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-U_jhXT_0Cbs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18f31e0b-0e8a-423d-8cbd-19f2c13c900c"
      },
      "source": [
        "# fonctions pour charger les ensembles de donnees\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_fashion_mnist_dataloaders(val_percentage=0.1, batch_size=1):\n",
        "  dataset = FashionMNIST(\"./dataset\", train=True,  download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "  dataset_test = FashionMNIST(\"./dataset\", train=False,  download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "  len_train = int(len(dataset) * (1.-val_percentage))\n",
        "  len_val = len(dataset) - len_train\n",
        "  dataset_train, dataset_val = random_split(dataset, [len_train, len_val])\n",
        "  data_loader_train = DataLoader(dataset_train, batch_size=batch_size,shuffle=True,num_workers=4)\n",
        "  data_loader_val   = DataLoader(dataset_val, batch_size=batch_size,shuffle=True,num_workers=4)\n",
        "  data_loader_test  = DataLoader(dataset_test, batch_size=batch_size,shuffle=True,num_workers=4)\n",
        "  return data_loader_train, data_loader_val, data_loader_test\n",
        "\n",
        "def reshape_input(x, y):\n",
        "    x = x.view(-1, 784)\n",
        "    y = torch.FloatTensor(len(y), 10).zero_().scatter_(1,y.view(-1,1),1)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "# call this once first to download the datasets\n",
        "_ = get_fashion_mnist_dataloaders()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./dataset/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 19.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./dataset/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./dataset/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./dataset/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 348kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./dataset/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./dataset/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./dataset/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 6.25MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./dataset/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./dataset/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./dataset/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 4.62MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./dataset/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./dataset/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9H5BnbgAOpio"
      },
      "source": [
        "# simple logger to track progress during training\n",
        "class Logger:\n",
        "    def __init__(self):\n",
        "        self.losses_train = []\n",
        "        self.losses_valid = []\n",
        "        self.accuracies_train = []\n",
        "        self.accuracies_valid = []\n",
        "\n",
        "    def log(self, accuracy_train=0, loss_train=0, accuracy_valid=0, loss_valid=0):\n",
        "        self.losses_train.append(loss_train)\n",
        "        self.accuracies_train.append(accuracy_train)\n",
        "        self.losses_valid.append(loss_valid)\n",
        "        self.accuracies_valid.append(accuracy_valid)\n",
        "\n",
        "    def plot_loss_and_accuracy(self, train=True, valid=True):\n",
        "\n",
        "        assert train and valid, \"Cannot plot accuracy because neither train nor valid.\"\n",
        "\n",
        "        figure, (ax1, ax2) = plt.subplots(nrows=1, ncols=2,\n",
        "                                            figsize=(12, 6))\n",
        "\n",
        "        if train:\n",
        "            ax1.plot(self.losses_train, label=\"Training\")\n",
        "            ax2.plot(self.accuracies_train, label=\"Training\")\n",
        "        if valid:\n",
        "            ax1.plot(self.losses_valid, label=\"Validation\")\n",
        "            ax1.set_title(\"CrossEntropy Loss\")\n",
        "            ax2.plot(self.accuracies_valid, label=\"Validation\")\n",
        "            ax2.set_title(\"Accuracy\")\n",
        "\n",
        "        for ax in figure.axes:\n",
        "            ax.set_xlabel(\"Epoch\")\n",
        "            ax.legend(loc='best')\n",
        "            ax.set_axisbelow(True)\n",
        "            ax.minorticks_on()\n",
        "            ax.grid(True, which=\"major\", linestyle='-')\n",
        "            ax.grid(True, which=\"minor\", linestyle='--', color='lightgrey', alpha=.4)\n",
        "\n",
        "    def print_last(self):\n",
        "        print(f\"Epoch {len(self.losses_train):2d}, \\\n",
        "                Train:loss={self.losses_train[-1]:.3f}, accuracy={self.accuracies_train[-1]*100:.1f}%, \\\n",
        "                Valid: loss={self.losses_valid[-1]:.3f}, accuracy={self.losses_valid[-1]*100:.1f}%\", flush=True)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAJ5iiRUZw3f"
      },
      "source": [
        "## Aperçu de l'ensemble de données FashionMnist"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "fK-eNmc8Zv2d",
        "outputId": "74256e66-ebaf-480c-bbb4-d40bcff50a5b"
      },
      "source": [
        "def plot_samples():\n",
        "  a, _, _ = get_fashion_mnist_dataloaders()\n",
        "  num_row = 2\n",
        "  num_col = 5# plot images\n",
        "  num_images = num_row * num_col\n",
        "  fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
        "  for i, (x,y) in enumerate(a):\n",
        "      if i >= num_images:\n",
        "        break\n",
        "      ax = axes[i//num_col, i%num_col]\n",
        "      x = (x.numpy().squeeze() * 255).astype(int)\n",
        "      y = y.numpy()[0]\n",
        "      ax.imshow(x, cmap='gray')\n",
        "      ax.set_title(f\"Label: {y}\")\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "plot_samples()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 750x400 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IficnxEMMcNo"
      },
      "source": [
        "# Partie 3 (20 points)\n",
        "\n",
        "Pour cette partie, vous pouvez travailler en groupes de 2, mais il faut écrire sa propre dérivation et soumettre son propre rapport. Si vous travaillez avec un partenaire, il faut indiquer leur nom dans votre rapport."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zmb_putke8pl"
      },
      "source": [
        "### Problème"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRioLmIDMcP8"
      },
      "source": [
        "![picture](https://drive.google.com/uc?id=17_N7pIrf5pypQKiUh5cM7SX6raZUBcJC)\n",
        "\n",
        "Considérons maintenant un réseau de neurones avec une couche d'entrée avec $D=784$ unités, $L$ couches cachées, chacune avec 300 unités et un vecteur de sortie $\\mathbf{y}$ de dimension $K$. Vous avez $i = 1, .., N$ exemples dans un ensemble d'apprentissage, où chaque ${\\bf x}_i \\in \\mathbb{R}^{784}$ est un vecteur de caractéristiques (features). $\\mathbf{y}$ est un vecteur du type *one-hot* -- un vecteur de zéros avec un seul 1 pour indiquer que la classe $C=k$ dans la dimension $k$. Par exemple, le vecteur $\\mathbf{y}=[0, 1, 0, \\cdots, 0]^T$ représente la deuxième classe. La fonction de perte est donnée par\n",
        "\\begin{equation}\n",
        "\\mathscr{L} = -\\sum_{i=1}^{N}\\sum_{k=1}^{K}y_{k,i}\\log (f_k( {\\bf x}_i )  )\n",
        "\\end{equation}\n",
        "\n",
        "La fonction d'activation de la couche finale a la forme  ${\\bf f} = [f_1, ..., f_K]$ donné par la fonction d'activation softmax:\n",
        "\\begin{equation}\n",
        "f_k( {\\bf a}^{(L+1)}({\\bf x}_i) ) = \\frac{\\exp(a_k^{(L+1)})}{\\sum_{c=1}^{K}\\exp(a_c^{(L+1)})}, \\;\\;\\;\\;\n",
        "\\nonumber\n",
        "\\end{equation}\n",
        "\n",
        "et les couches cachées utilisent une fonction d'activation de type ReLU:\n",
        "\\begin{equation}\n",
        "  {\\bf h}^{(l)}({\\bf a}^{(l)}({\\bf x}_i)) = \\text{ReLU}({\\bf a}^{(l)}({\\bf x}_i) = \\max\\Big(0, \\, \\, {\\bf a}^{(l)}({\\bf x}_i)\\Big)\n",
        "\\end{equation}\n",
        "\n",
        "où ${\\bf a}^{(l)}$ est le vecteur résultant du calcul de la préactivation habituelle ${\\bf a}^{(l)}={\\bf W}^{(l)}{\\bf h}^{(l-1)} + {\\bf b}^{(l)}$, qui pourrait être simplifiée à ${\\boldsymbol \\theta}^{(l)}\\tilde{\\bf h}^{(l-1)}$ en utilisant l'astuce de définir $\\tilde{\\bf h}$ comme ${\\bf h}$ avec un 1 concaténé à la fin du vecteur.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzMpz3Zse0t9"
      },
      "source": [
        "### Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wK8gnygxMcSh"
      },
      "source": [
        "* a) (10 points) Donnez le pseudocode incluant des *calculs matriciels—vectoriels* détaillés pour l'algorithme de rétropropagation pour calculer le gradient pour les paramètres de chaque couche **étant donné un exemple d'entraînement**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y431_w4gMcX2"
      },
      "source": [
        "* b) (15 points)\n",
        "Implémentez l'optimisation basée sur le gradient de ce réseau en Pytorch.\n",
        "Utilisez le code squelette ci-dessous comme point de départ et implémentez les mathématiques de l'algorithme de rétropropagation que vous avez décrit à la question précédente. Comparez vos gradients et votre optimisation avec le même modèle optimisé avec Autograd. Lequel est le plus rapide ? Proposez quelques expériences. Utilisez encore l'ensemble de données de Fashion MNIST (voir Partie 2). **Comparez différents modèles ayant différentes largeurs (nombre d'unités) et profondeurs (nombre de couches)**. Ici encore, n'utilisez l'ensemble de test que pour votre expérience finale lorsque vous pensez avoir obtenu votre meilleur modèle.\n",
        "\n",
        "\n",
        "**IMPORTANT**\n",
        "\n",
        "L'objectif du TP est de vous faire implémenter la rétropropagation à la main. L'objectif est d'implémenter un modèle de classification logistique ainsi que son entainement en utilisant uniquement des opérations matricielles de base fournies par PyTorch e.g. torch.sum(), torch.matmul(), etc. **Une fois que vous avez implémenté votre modèle, vous devez le comparer avec un modèle construit en utilisant les capacités de pytorch qui permettent une différenciation automatique. Autrement dit, pour la deuxième implémentation, vous pouvez utilisertorch.nn, torch.autograd ou à la méthode .backward().** Vous pouvez utiliser l’implémentation de votre choix pour explorer différentes architectures de modèles."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Votre pseudocode:"
      ],
      "metadata": {
        "id": "F1mpuG2cwER-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Algorithme de rétropopagation dans un réseau de neurones pour un exemple $\\tilde{x}_i$ :\n",
        "\n",
        "\n",
        "#### Les calules justifiant cet algorithme on été effectués dans le rapport joint.\n",
        "\n",
        "$x_i$ est noté simplement $x$ dans l'algo\n",
        "\n",
        "***Require***\n",
        "\n",
        ">$L$ (nombre de couches cachées),\n",
        "\n",
        ">$x$ (exemple de donnée d'entrainement),\n",
        "\n",
        ">$\\theta^{(l)}$ (poids des couches),\n",
        "\n",
        "> $\\tilde{h}^{(l)}$ (post-activation augmentée : issue de la forward prop de x à la couche $l$),\n",
        "\n",
        "> $a^{(l)}$ (pré-activation : issue de la forward prop de x à la couche $l$), $\\text{lr}$ (learning rate)\n",
        "\n",
        "**Initialisation *texte en italique* :**\n",
        "\n",
        "> $\\Delta^{(L+1)} \\gets y - y_{\\text{pred}}$\n",
        "\n",
        "> $\\text{grad}^{(L+1)} \\gets -\\Delta^{(L+1)} \\cdot (\\tilde{h}^{(L)})^T$\n",
        "\n",
        " ***For*** $l = L$ ***to*** $1$ :\n",
        "\n",
        "> $d^{(l)} \\gets \\text{relu_backward}(a^{(l)})$\n",
        "\n",
        ">$W^{(l+1)} \\gets \\theta^{(l+1)}$ sans la dernière ligne\n",
        "\n",
        ">$\\Delta^{(l)} \\gets d^{(l)} \\circ (W^{(l+1)T} \\cdot \\Delta^{(l+1)})$\n",
        "\n",
        ">$\\text{grad}^{(l)} \\gets -\\Delta^{(l)} \\cdot (\\tilde{h}^{(l-1)})^T$\n",
        "\n",
        "  **EndFor**\n",
        "\n",
        "  ***For*** $l = L+1$ ***to*** $1$ :\n",
        "  >$\\theta^{(l)} \\gets \\theta^{(l)} - \\text{lr} \\cdot \\text{grad}^{(l)}$\n",
        "  \n",
        "  **EndFor**"
      ],
      "metadata": {
        "id": "qY2X9goYwMDs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fonctions à compléter"
      ],
      "metadata": {
        "id": "SIQJD-TRwEdo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzIQ0S4qDPKJ"
      },
      "source": [
        "''' Les fonctions dans cette cellule peuvent avoir les mêmes déclarations que celles de la partie 2'''\n",
        "def accuracy(y, y_pred) :\n",
        "    # todo : nombre d'éléments à classifier.\n",
        "    card_D = len(y)\n",
        "\n",
        "    # todo : calcul du nombre d'éléments bien classifiés.\n",
        "    y_pred_classes = torch.argmax(y_pred, dim=1)  # Indices des classes prédictes\n",
        "    y_true_classes = torch.argmax(y, dim=1)       # Indices des classes vraies\n",
        "    card_C = (y_pred_classes == y_true_classes).sum().item()\n",
        "\n",
        "    # todo : calcul de la précision de classification.\n",
        "    acc = card_C / card_D\n",
        "\n",
        "    return acc, (card_C, card_D)\n",
        "\n",
        "def accuracy_and_loss_whole_dataset(data_loader, model):\n",
        "    cardinal = 0\n",
        "    loss     = 0.\n",
        "    n_accurate_preds  = 0.\n",
        "\n",
        "    for x, y in data_loader:\n",
        "        x, y = reshape_input(x, y)\n",
        "        y_pred                = model.forward(x)\n",
        "        xentrp                = cross_entropy(y, y_pred)\n",
        "        _, (n_acc, n_samples) = accuracy(y, y_pred)\n",
        "\n",
        "        cardinal = cardinal + n_samples\n",
        "        loss     = loss + xentrp\n",
        "        n_accurate_preds  = n_accurate_preds + n_acc\n",
        "\n",
        "    loss = loss / float(cardinal)\n",
        "    acc  = n_accurate_preds / float(cardinal)\n",
        "\n",
        "    return acc, loss\n",
        "\n",
        "def inputs_tilde(x, axis=-1):\n",
        "    # augments the inputs `x` with ones along `axis`\n",
        "    # todo : implémenter code ici.\n",
        "    # On concatène une colonne de 1 pour le biais\n",
        "    ones = torch.ones(x.shape[0], 1)\n",
        "    x_tilde = torch.cat((x, ones), dim=axis)\n",
        "    return x_tilde\n",
        "\n",
        "def softmax(x, axis=-1):\n",
        "    # assurez vous que la fonction est numeriquement stable\n",
        "    # e.g. softmax(np.array([1000, 10000, 100000], ndim=2))\n",
        "    # todo : calcul des valeurs de softmax(x)\n",
        "\n",
        "    #On assume que les d'entrées sont les suivantes :\n",
        "\n",
        "    # x = [x_1, x_2 ... x_batchSize]\n",
        "    # x_1 = [w_observ1_classe0 . x_observ1_classe0 + b_observ1_classe0, w_observ1_classe1 . x_observ1_classe1 + b_observ1_classe1, ...]  (logit pour les différentes classes)\n",
        "\n",
        "    # Pour la stabilité comme softmax(x_i) = [exp(x_i[0]), exp(x_i[1]), ... , exp(x_i[K])] / sum(exp(x_i))\n",
        "    # si on divise en haut et en base bar exp(max(x_i)) on toujours l'égalité donc on peut juste retranché le logit le plus grand pour chaque batch afin d'avoir aucun exponentiel > 1 (non stable) à caculer\n",
        "\n",
        "    x_max = torch.amax(x, dim=axis, keepdim=True)\n",
        "    x_exp = torch.exp(x - x_max)\n",
        "    sum_exp = torch.sum(x_exp, dim=axis, keepdim=True)\n",
        "    values = x_exp / sum_exp\n",
        "    return values\n",
        "\n",
        "def cross_entropy(y, y_pred):\n",
        "    # todo : calcul de la valeur d'entropie croisée.\n",
        "    loss = -torch.sum(y * torch.log(y_pred + 1e-8)) / y.shape[0]\n",
        "    return loss\n",
        "\n",
        "def softmax_cross_entropy_backward(y, y_pred):\n",
        "     # todo : calcul de la valeur du gradient de l'entropie croisée composée avec `softmax`\n",
        "     # On a L = -\\sum_k y_k log(softmax_k) et on a montré que ce gradiant par rapport aux pré-activation (les a^(L+1) avant le softmax) donne\n",
        "     # dL/da^(L+1) = -\\Delta^(L+1) = -(y-y_pred)\n",
        "     values = -(y-y_pred)\n",
        "     return values\n",
        "\n",
        "def relu_forward(x):\n",
        "    # todo : calcul des valeurs de relu(x)\n",
        "    # Relu c'est juste le max valeurs par valeurs entre 0 et les valeurs de x, on évite d'utiliser torch.relu pour essayer de tout faire le plus possible à la main pour bien comprendre\n",
        "    values = torch.maximum(x, torch.tensor(0.0))\n",
        "    return values\n",
        "\n",
        "def relu_backward(x):\n",
        "    # todo : calcul des valeurs du gradient de la fonction `relu`\n",
        "    values = (x > 0).float()  # 1 si x > 0, sinon 0\n",
        "    return values\n",
        "\n",
        "\n",
        "# Model est une classe representant votre reseaux de neuronnes\n",
        "class MLPModel:\n",
        "    def __init__(self, n_features, n_hidden_features, n_hidden_layers, n_classes):\n",
        "        self.n_features        = n_features\n",
        "        self.n_hidden_features = n_hidden_features\n",
        "        self.n_hidden_layers   = n_hidden_layers\n",
        "        self.n_classes         = n_classes\n",
        "\n",
        "        # todo : initialiser la liste des paramètres Teta de l'estimateur.\n",
        "\n",
        "\n",
        "        self.params = []\n",
        "\n",
        "        # première couche theta^(1) : (n_hidden_features, n_features+1)\n",
        "        self.params.append(torch.randn(n_hidden_features, n_features+1))\n",
        "\n",
        "        # Couches cachées theta^(l) : (n_hidden_features, n_hidden_features + 1)\n",
        "        for _ in range(n_hidden_layers - 1):\n",
        "            self.params.append(torch.randn(n_hidden_features, n_hidden_features + 1))\n",
        "\n",
        "        # dernière couche theta^(L+1) : (n_classes, n_hidden_features + 1)\n",
        "        self.params.append(torch.randn(n_classes, n_hidden_features + 1))\n",
        "\n",
        "\n",
        "\n",
        "        print(f\"Teta params={[p.shape for p in self.params]}\")\n",
        "\n",
        "        self.a = None # liste contenant le resultat des multiplications matricielles\n",
        "        self.h = None # liste contenant le resultat des fonctions d'activations\n",
        "\n",
        "        self.t = 0\n",
        "        self.m_t = [] # pour Adam: moyennes mobiles du gradient\n",
        "        self.v_t = [] # pour Adam: moyennes mobiles du carré du gradient\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "      self.a = []\n",
        "      self.h = []\n",
        "      h = inputs_tilde(x, 1)\n",
        "      h = torch.transpose(h, 0, 1)\n",
        "      self.h.append(h)\n",
        "      l = 0\n",
        "      for theta in self.params[:-1]:\n",
        "        l+= 1\n",
        "        a = torch.matmul(theta, h)\n",
        "        self.a.append(a)\n",
        "        h = relu_forward(a)\n",
        "        h = torch.transpose(h, 0, 1)\n",
        "        h = inputs_tilde(h, 1)\n",
        "        h = torch.transpose(h, 0, 1)\n",
        "\n",
        "        self.h.append(h)\n",
        "      a = torch.matmul(self.params[-1], h)\n",
        "      self.a.append(a)\n",
        "      outputs = softmax(torch.transpose(a,0,1), 1)\n",
        "\n",
        "      return outputs\n",
        "\n",
        "    def backward(self, y, y_pred):\n",
        "      # todo : implémenter calcul des gradients.\n",
        "\n",
        "      #On fait les calcules comme si on avait un batch de taille 1 et donc des vecteurs colonnes pour les couches\n",
        "      y_pred = torch.transpose(y_pred,0,1)\n",
        "      y = torch.transpose(y,0,1)\n",
        "\n",
        "\n",
        "      delta = - softmax_cross_entropy_backward(y, y_pred)\n",
        "      deltas = [delta]\n",
        "      grads = []\n",
        "      for i in range(len(self.params)):\n",
        "        l = len(self.params)-1 - i\n",
        "        if l != len(self.params)-1:\n",
        "          # calculer delta^(l) en fonction du suivant delta^(l+1)\n",
        "          dl = relu_backward(self.a[l]) #vecteur issue de la matrice D^(l) issue de dh^(l)/da^(l)\n",
        "          W_next = self.params[l+1][:,:-1]\n",
        "          mult = torch.matmul(torch.transpose(W_next, 0,1), delta)\n",
        "          delta = dl * mult\n",
        "          deltas.append(delta)\n",
        "\n",
        "        grad = -torch.matmul(delta, torch.transpose(self.h[l],0,1))   #pas besoin de mettre h[l-1] car les h sont indicé en décalé par rapport au W et a car on a mis l'input tilde dans h au début et pas l'output\n",
        "        # print(f\"grad.shape : {grad.shape}\", \"\\n\")\n",
        "        grads.append(grad)\n",
        "\n",
        "      list.reverse(grads)\n",
        "      return grads\n",
        "\n",
        "    def sgd_update(self, lr, grads):\n",
        "      # TODO : implémenter mise à jour des paramètres ici.\n",
        "\n",
        "      for i in range(len(self.params)):\n",
        "        self.params[i] -= lr * grads[i]\n",
        "\n",
        "\n",
        "    def adam_update(self, lr, grads):\n",
        "      # TODO : implémenter mise à jour des paramètres ici.\n",
        "      beta1 = 0.9\n",
        "      beta2 = 0.999\n",
        "\n",
        "      if not self.m_t:\n",
        "        self.m_t = [torch.zeros_like(g) for g in grads]\n",
        "        self.v_t = [torch.zeros_like(g) for g in grads]\n",
        "\n",
        "\n",
        "      self.t += 1\n",
        "      for i in range(len(self.params)):\n",
        "        self.m_t[i] = beta1 * self.m_t[i] + (1 - beta1) * grads[i]\n",
        "        self.v_t[i] = beta2 * self.v_t[i] + (1 - beta2) * (grads[i]**2)\n",
        "        m = self.m_t[i] / (1 - beta1**self.t)\n",
        "        v = self.v_t[i] / (1 - beta2**self.t)\n",
        "        self.params[i] -= lr * m / (torch.sqrt(v) + 1e-8)\n",
        "\n",
        "def train(model, lr=0.1, nb_epochs=10, sgd=True, data_loader_train=None, data_loader_val=None):\n",
        "    best_model = None\n",
        "    best_val_accuracy = 0\n",
        "    logger = Logger()\n",
        "\n",
        "    for epoch in range(nb_epochs+1):\n",
        "\n",
        "        # at epoch 0 evaluate random initial model\n",
        "        #   then for subsequent epochs, do optimize before evaluation.\n",
        "        if epoch > 0:\n",
        "            for x, y in data_loader_train:\n",
        "                x, y = reshape_input(x, y)\n",
        "\n",
        "                y_pred = model.forward(x)\n",
        "                grads  = model.backward(y, y_pred)\n",
        "                if sgd:\n",
        "                  model.sgd_update(lr, grads)\n",
        "                else:\n",
        "                  model.adam_update(lr, grads)\n",
        "\n",
        "        accuracy_train, loss_train = accuracy_and_loss_whole_dataset(data_loader_train, model)\n",
        "        accuracy_val, loss_val = accuracy_and_loss_whole_dataset(data_loader_val, model)\n",
        "\n",
        "        if accuracy_val > best_val_accuracy:\n",
        "          best_val_accuracy = accuracy_val\n",
        "          best_model = model\n",
        "\n",
        "        logger.log(accuracy_train, loss_train, accuracy_val, loss_val)\n",
        "        print(f\"Epoch {epoch:2d}, \\\n",
        "                Train:loss={loss_train:.3f}, accuracy={accuracy_train*100:.1f}%, \\\n",
        "                Valid: loss={loss_val:.3f}, accuracy={accuracy_val*100:.1f}%\", flush=True)\n",
        "\n",
        "    return best_model, best_val_accuracy, logger"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Évaluation"
      ],
      "metadata": {
        "id": "tIe9DFvPwuQg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SGD: Recherche d'hyperparamètres"
      ],
      "metadata": {
        "id": "ml5jUvG9AUXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SGD\n",
        "# Montrez les résultats pour différents nombre de couche, e.g. 1, 3, 5, et différent nombres de neurone, e.g. 25, 100, 300, 500, 1000.\n",
        "depth_list = [1,3,5]   # Define ranges in a list\n",
        "width_list = [25, 100, 300, 500, 1000]   # Define ranges in a list\n",
        "lr = 0.001           # Some value\n",
        "batch_size = 20   # Some value\n",
        "\n",
        "with torch.no_grad():\n",
        "  for depth in depth_list:\n",
        "    for width in width_list:\n",
        "      print(\"------------------------------------------------------------------\")\n",
        "      print(\"Training model with a depth of {0} layers and a width of {1} units\".format(depth, width))\n",
        "      data_loader_train, data_loader_val, data_loader_test = get_fashion_mnist_dataloaders(val_percentage=0.1, batch_size=batch_size)\n",
        "\n",
        "      MLP_model = MLPModel(n_features=784, n_hidden_features=width, n_hidden_layers=depth, n_classes=10)\n",
        "      _, val_accuracy, _ = train(MLP_model,lr=lr, nb_epochs=5, sgd=True, data_loader_train=data_loader_train, data_loader_val=data_loader_val)\n",
        "      print(f\"validation accuracy = {val_accuracy*100:.3f}\")"
      ],
      "metadata": {
        "id": "fe7hyN63AUXL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49ce2981-42f4-4cd6-d355-1d033f995929"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------\n",
            "Training model with a depth of 1 layers and a width of 25 units\n",
            "Teta params=[torch.Size([25, 785]), torch.Size([10, 26])]\n",
            "Epoch  0,                 Train:loss=0.811, accuracy=8.1%,                 Valid: loss=0.812, accuracy=8.0%\n",
            "Epoch  1,                 Train:loss=0.054, accuracy=62.4%,                 Valid: loss=0.055, accuracy=62.5%\n",
            "Epoch  2,                 Train:loss=0.044, accuracy=66.5%,                 Valid: loss=0.044, accuracy=66.9%\n",
            "Epoch  3,                 Train:loss=0.041, accuracy=68.7%,                 Valid: loss=0.041, accuracy=68.7%\n",
            "Epoch  4,                 Train:loss=0.040, accuracy=69.6%,                 Valid: loss=0.041, accuracy=70.2%\n",
            "Epoch  5,                 Train:loss=0.039, accuracy=69.7%,                 Valid: loss=0.039, accuracy=69.7%\n",
            "validation accuracy = 70.233\n",
            "------------------------------------------------------------------\n",
            "Training model with a depth of 1 layers and a width of 100 units\n",
            "Teta params=[torch.Size([100, 785]), torch.Size([10, 101])]\n",
            "Epoch  0,                 Train:loss=0.818, accuracy=10.0%,                 Valid: loss=0.809, accuracy=10.9%\n",
            "Epoch  1,                 Train:loss=0.055, accuracy=72.7%,                 Valid: loss=0.060, accuracy=70.9%\n",
            "Epoch  2,                 Train:loss=0.040, accuracy=75.4%,                 Valid: loss=0.043, accuracy=74.3%\n",
            "Epoch  3,                 Train:loss=0.034, accuracy=77.2%,                 Valid: loss=0.037, accuracy=75.6%\n",
            "Epoch  4,                 Train:loss=0.032, accuracy=78.4%,                 Valid: loss=0.036, accuracy=76.4%\n",
            "Epoch  5,                 Train:loss=0.031, accuracy=78.8%,                 Valid: loss=0.034, accuracy=77.6%\n",
            "validation accuracy = 77.633\n",
            "------------------------------------------------------------------\n",
            "Training model with a depth of 1 layers and a width of 300 units\n",
            "Teta params=[torch.Size([300, 785]), torch.Size([10, 301])]\n",
            "Epoch  0,                 Train:loss=0.810, accuracy=10.5%,                 Valid: loss=0.811, accuracy=10.5%\n",
            "Epoch  1,                 Train:loss=0.088, accuracy=77.2%,                 Valid: loss=0.095, accuracy=76.0%\n",
            "Epoch  2,                 Train:loss=0.052, accuracy=79.4%,                 Valid: loss=0.059, accuracy=78.2%\n",
            "Epoch  3,                 Train:loss=0.043, accuracy=80.3%,                 Valid: loss=0.050, accuracy=78.9%\n",
            "Epoch  4,                 Train:loss=0.037, accuracy=81.7%,                 Valid: loss=0.046, accuracy=80.2%\n",
            "Epoch  5,                 Train:loss=0.033, accuracy=82.0%,                 Valid: loss=0.042, accuracy=79.7%\n",
            "validation accuracy = 80.167\n",
            "------------------------------------------------------------------\n",
            "Training model with a depth of 1 layers and a width of 500 units\n",
            "Teta params=[torch.Size([500, 785]), torch.Size([10, 501])]\n",
            "Epoch  0,                 Train:loss=0.824, accuracy=9.9%,                 Valid: loss=0.826, accuracy=9.8%\n",
            "Epoch  1,                 Train:loss=0.099, accuracy=81.4%,                 Valid: loss=0.106, accuracy=80.8%\n",
            "Epoch  2,                 Train:loss=0.064, accuracy=81.6%,                 Valid: loss=0.073, accuracy=79.8%\n",
            "Epoch  3,                 Train:loss=0.049, accuracy=82.1%,                 Valid: loss=0.058, accuracy=80.6%\n",
            "Epoch  4,                 Train:loss=0.039, accuracy=83.0%,                 Valid: loss=0.050, accuracy=80.6%\n",
            "Epoch  5,                 Train:loss=0.036, accuracy=83.9%,                 Valid: loss=0.049, accuracy=81.0%\n",
            "validation accuracy = 81.050\n",
            "------------------------------------------------------------------\n",
            "Training model with a depth of 1 layers and a width of 1000 units\n",
            "Teta params=[torch.Size([1000, 785]), torch.Size([10, 1001])]\n",
            "Epoch  0,                 Train:loss=0.852, accuracy=6.8%,                 Valid: loss=0.847, accuracy=7.4%\n",
            "Epoch  1,                 Train:loss=0.125, accuracy=81.9%,                 Valid: loss=0.133, accuracy=81.3%\n",
            "Epoch  2,                 Train:loss=0.083, accuracy=83.5%,                 Valid: loss=0.097, accuracy=82.2%\n",
            "Epoch  3,                 Train:loss=0.060, accuracy=84.6%,                 Valid: loss=0.080, accuracy=82.6%\n",
            "Epoch  4,                 Train:loss=0.047, accuracy=85.7%,                 Valid: loss=0.068, accuracy=83.4%\n",
            "Epoch  5,                 Train:loss=0.039, accuracy=86.5%,                 Valid: loss=0.062, accuracy=84.0%\n",
            "validation accuracy = 83.967\n",
            "------------------------------------------------------------------\n",
            "Training model with a depth of 3 layers and a width of 25 units\n",
            "Teta params=[torch.Size([25, 785]), torch.Size([25, 26]), torch.Size([25, 26]), torch.Size([10, 26])]\n",
            "Epoch  0,                 Train:loss=0.871, accuracy=5.1%,                 Valid: loss=0.865, accuracy=5.6%\n",
            "Epoch  1,                 Train:loss=0.098, accuracy=23.7%,                 Valid: loss=0.099, accuracy=23.4%\n",
            "Epoch  2,                 Train:loss=0.081, accuracy=33.8%,                 Valid: loss=0.083, accuracy=32.7%\n",
            "Epoch  3,                 Train:loss=0.073, accuracy=39.1%,                 Valid: loss=0.074, accuracy=38.3%\n",
            "Epoch  4,                 Train:loss=0.070, accuracy=41.4%,                 Valid: loss=0.070, accuracy=40.5%\n",
            "Epoch  5,                 Train:loss=0.068, accuracy=42.5%,                 Valid: loss=0.068, accuracy=41.9%\n",
            "validation accuracy = 41.900\n",
            "------------------------------------------------------------------\n",
            "Training model with a depth of 3 layers and a width of 100 units\n",
            "Teta params=[torch.Size([100, 785]), torch.Size([100, 101]), torch.Size([100, 101]), torch.Size([10, 101])]\n",
            "Epoch  0,                 Train:loss=0.821, accuracy=10.7%,                 Valid: loss=0.821, accuracy=10.8%\n",
            "Epoch  1,                 Train:loss=nan, accuracy=10.0%,                 Valid: loss=nan, accuracy=10.0%\n",
            "Epoch  2,                 Train:loss=nan, accuracy=10.0%,                 Valid: loss=nan, accuracy=10.0%\n",
            "Epoch  3,                 Train:loss=nan, accuracy=10.0%,                 Valid: loss=nan, accuracy=10.0%\n",
            "Epoch  4,                 Train:loss=nan, accuracy=10.0%,                 Valid: loss=nan, accuracy=10.0%\n",
            "Epoch  5,                 Train:loss=nan, accuracy=10.0%,                 Valid: loss=nan, accuracy=10.0%\n",
            "validation accuracy = 10.767\n",
            "------------------------------------------------------------------\n",
            "Training model with a depth of 3 layers and a width of 300 units\n",
            "Teta params=[torch.Size([300, 785]), torch.Size([300, 301]), torch.Size([300, 301]), torch.Size([10, 301])]\n",
            "Epoch  0,                 Train:loss=0.833, accuracy=9.5%,                 Valid: loss=0.829, accuracy=10.0%\n",
            "Epoch  1,                 Train:loss=nan, accuracy=10.0%,                 Valid: loss=nan, accuracy=10.4%\n",
            "Epoch  2,                 Train:loss=nan, accuracy=10.0%,                 Valid: loss=nan, accuracy=10.4%\n",
            "Epoch  3,                 Train:loss=nan, accuracy=10.0%,                 Valid: loss=nan, accuracy=10.4%\n",
            "Epoch  4,                 Train:loss=nan, accuracy=10.0%,                 Valid: loss=nan, accuracy=10.4%\n",
            "Epoch  5,                 Train:loss=nan, accuracy=10.0%,                 Valid: loss=nan, accuracy=10.4%\n",
            "validation accuracy = 10.433\n",
            "------------------------------------------------------------------\n",
            "Training model with a depth of 3 layers and a width of 500 units\n",
            "Teta params=[torch.Size([500, 785]), torch.Size([500, 501]), torch.Size([500, 501]), torch.Size([10, 501])]\n",
            "Epoch  0,                 Train:loss=0.784, accuracy=14.9%,                 Valid: loss=0.787, accuracy=14.5%\n",
            "Epoch  1,                 Train:loss=nan, accuracy=10.0%,                 Valid: loss=nan, accuracy=10.3%\n",
            "Epoch  2,                 Train:loss=nan, accuracy=10.0%,                 Valid: loss=nan, accuracy=10.3%\n",
            "Epoch  3,                 Train:loss=nan, accuracy=10.0%,                 Valid: loss=nan, accuracy=10.3%\n",
            "Epoch  4,                 Train:loss=nan, accuracy=10.0%,                 Valid: loss=nan, accuracy=10.3%\n",
            "Epoch  5,                 Train:loss=nan, accuracy=10.0%,                 Valid: loss=nan, accuracy=10.3%\n",
            "validation accuracy = 14.550\n",
            "------------------------------------------------------------------\n",
            "Training model with a depth of 3 layers and a width of 1000 units\n",
            "Teta params=[torch.Size([1000, 785]), torch.Size([1000, 1001]), torch.Size([1000, 1001]), torch.Size([10, 1001])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Tableau pour la précision sur l'ensemble de validation**\n",
        "N.B. que les lignes correspondent aux nombre de couche et les colonnes correspondent au nombre de neurone dans chaque couche. Les valeurs ci-dessous sont donné comme exemples; remplacez-les par les valeurs que vous avez utilisées pour votre recherche d'hyperparamètres.\n",
        "\n",
        "depth\\width  | 25 | 100 | 300 | 500 | 1000\n",
        "-------------------|------------------|------------------|------------------|------------------|------------------|\n",
        "**1**   | -  | - | - | - | - |\n",
        "**3** | -  | - | - | - | - |\n",
        "**5**  | -  | - | - | - | - |"
      ],
      "metadata": {
        "id": "QnDMqFapAUXM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SGD: Analyse du meilleur modèle"
      ],
      "metadata": {
        "id": "2wuN-uw7AUXN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CShZ3VB0AUXN"
      },
      "source": [
        "# SGD\n",
        "# Montrez les résultats pour la meilleure configuration trouvez ci-dessus.\n",
        "depth = None    # TODO: Vous devez modifier cette valeur avec la meilleur que vous avez eu.\n",
        "width = None    # TODO: Vous devez modifier cette valeur avec la meilleur que vous avez eu.\n",
        "lr = 0.05           # Some value\n",
        "batch_size = 500   # Some value\n",
        "\n",
        "with torch.no_grad():\n",
        "  data_loader_train, data_loader_val, data_loader_test = get_fashion_mnist_dataloaders(val_percentage=0.1, batch_size=batch_size)\n",
        "\n",
        "  MLP_model = MLPModel(n_features=784, n_hidden_features=width, n_hidden_layers=depth, n_classes=10)\n",
        "  best_model, best_val_accuracy, logger = train(MLP_model,lr=lr, nb_epochs=5, sgd=True,\n",
        "                                                data_loader_train=data_loader_train, data_loader_val=data_loader_val)\n",
        "  logger.plot_loss_and_accuracy()\n",
        "  print(f\"Best validation accuracy = {best_val_accuracy*100:.3f}\")\n",
        "\n",
        "  accuracy_test, loss_test = accuracy_and_loss_whole_dataset(data_loader_test, best_model)\n",
        "print(\"Evaluation of the best training model over test set\")\n",
        "print(\"------\")\n",
        "print(f\"Loss : {loss_test:.3f}\")\n",
        "print(f\"Accuracy : {accuracy_test*100.:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adam: Recherche d'hyperparamètres\n",
        "\n",
        "Implémentez Adam, répétez les deux étapes précédentes (recherche d'hyperparamètres et analyse du meilleur modèle) cette fois en utilisat Adam, et comparez les performances finales avec votre meilleur modèle SGD."
      ],
      "metadata": {
        "id": "_tPLgZriAUXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ADAM\n",
        "# Montrez les résultats pour différents nombre de couche, e.g. 1, 3, 5, et différent nombres de neurone, e.g. 25, 100, 300, 500, 1000.\n",
        "depth_list = [1,3,5]   # Define ranges in a list\n",
        "width_list = [25, 100, 300, 500, 1000]   # Define ranges in a list\n",
        "lr = 0.05           # Some value\n",
        "batch_size = 500   # Some value\n",
        "\n",
        "with torch.no_grad():\n",
        "  for depth in depth_list:\n",
        "    for width in width_list:\n",
        "      print(\"------------------------------------------------------------------\")\n",
        "      print(\"Training model with a depth of {0} layers and a width of {1} units\".format(depth, width))\n",
        "      data_loader_train, data_loader_val, data_loader_test = get_fashion_mnist_dataloaders(val_percentage=0.1, batch_size=batch_size)\n",
        "\n",
        "      MLP_model = MLPModel(n_features=784, n_hidden_features=width, n_hidden_layers=depth, n_classes=10)\n",
        "      _, val_accuracy, _ = train(MLP_model, lr=lr, nb_epochs=5, sgd=False, data_loader_train=data_loader_train, data_loader_val=data_loader_val)\n",
        "      print(f\"validation accuracy = {val_accuracy*100:.3f}\")"
      ],
      "metadata": {
        "id": "tEVOh1r7AUXO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bb8c3d4-40d1-4319-9c6c-9bf46e0fbf06"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------\n",
            "Training model with a depth of 1 layers and a width of 25 units\n",
            "Teta params=[torch.Size([25, 785]), torch.Size([10, 26])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  0,                 Train:loss=0.005, accuracy=10.5%,                 Valid: loss=0.005, accuracy=10.3%\n",
            "Epoch  1,                 Train:loss=0.001, accuracy=72.0%,                 Valid: loss=0.001, accuracy=71.9%\n",
            "Epoch  2,                 Train:loss=0.001, accuracy=79.4%,                 Valid: loss=0.001, accuracy=78.9%\n",
            "Epoch  3,                 Train:loss=0.001, accuracy=80.9%,                 Valid: loss=0.001, accuracy=80.9%\n",
            "Epoch  4,                 Train:loss=0.001, accuracy=81.9%,                 Valid: loss=0.001, accuracy=81.7%\n",
            "Epoch  5,                 Train:loss=0.001, accuracy=82.7%,                 Valid: loss=0.001, accuracy=82.0%\n",
            "validation accuracy = 82.033\n",
            "------------------------------------------------------------------\n",
            "Training model with a depth of 1 layers and a width of 100 units\n",
            "Teta params=[torch.Size([100, 785]), torch.Size([10, 101])]\n",
            "Epoch  0,                 Train:loss=0.005, accuracy=13.8%,                 Valid: loss=0.005, accuracy=14.2%\n",
            "Epoch  1,                 Train:loss=0.001, accuracy=77.6%,                 Valid: loss=0.001, accuracy=77.3%\n",
            "Epoch  2,                 Train:loss=0.001, accuracy=83.9%,                 Valid: loss=0.001, accuracy=83.2%\n",
            "Epoch  3,                 Train:loss=0.001, accuracy=83.6%,                 Valid: loss=0.001, accuracy=82.6%\n",
            "Epoch  4,                 Train:loss=0.001, accuracy=85.8%,                 Valid: loss=0.001, accuracy=84.7%\n",
            "Epoch  5,                 Train:loss=0.001, accuracy=86.3%,                 Valid: loss=0.001, accuracy=85.3%\n",
            "validation accuracy = 85.333\n",
            "------------------------------------------------------------------\n",
            "Training model with a depth of 1 layers and a width of 300 units\n",
            "Teta params=[torch.Size([300, 785]), torch.Size([10, 301])]\n",
            "Epoch  0,                 Train:loss=0.005, accuracy=11.5%,                 Valid: loss=0.005, accuracy=11.4%\n",
            "Epoch  1,                 Train:loss=0.001, accuracy=79.4%,                 Valid: loss=0.001, accuracy=78.8%\n",
            "Epoch  2,                 Train:loss=0.001, accuracy=83.9%,                 Valid: loss=0.001, accuracy=82.3%\n",
            "Epoch  3,                 Train:loss=0.001, accuracy=84.3%,                 Valid: loss=0.001, accuracy=83.0%\n",
            "Epoch  4,                 Train:loss=0.001, accuracy=86.2%,                 Valid: loss=0.001, accuracy=84.4%\n",
            "Epoch  5,                 Train:loss=0.001, accuracy=86.3%,                 Valid: loss=0.001, accuracy=84.5%\n",
            "validation accuracy = 84.500\n",
            "------------------------------------------------------------------\n",
            "Training model with a depth of 1 layers and a width of 500 units\n",
            "Teta params=[torch.Size([500, 785]), torch.Size([10, 501])]\n",
            "Epoch  0,                 Train:loss=0.005, accuracy=10.7%,                 Valid: loss=0.005, accuracy=10.6%\n",
            "Epoch  1,                 Train:loss=0.001, accuracy=82.3%,                 Valid: loss=0.001, accuracy=82.9%\n",
            "Epoch  2,                 Train:loss=0.001, accuracy=82.7%,                 Valid: loss=0.001, accuracy=82.8%\n",
            "Epoch  3,                 Train:loss=0.001, accuracy=85.6%,                 Valid: loss=0.001, accuracy=85.6%\n",
            "Epoch  4,                 Train:loss=0.001, accuracy=85.3%,                 Valid: loss=0.001, accuracy=85.0%\n",
            "Epoch  5,                 Train:loss=0.001, accuracy=84.4%,                 Valid: loss=0.001, accuracy=84.5%\n",
            "validation accuracy = 85.633\n",
            "------------------------------------------------------------------\n",
            "Training model with a depth of 1 layers and a width of 1000 units\n",
            "Teta params=[torch.Size([1000, 785]), torch.Size([10, 1001])]\n",
            "Epoch  0,                 Train:loss=0.005, accuracy=16.6%,                 Valid: loss=0.005, accuracy=16.3%\n",
            "Epoch  1,                 Train:loss=0.001, accuracy=79.2%,                 Valid: loss=0.001, accuracy=78.8%\n",
            "Epoch  2,                 Train:loss=0.001, accuracy=84.8%,                 Valid: loss=0.001, accuracy=84.1%\n",
            "Epoch  3,                 Train:loss=0.001, accuracy=84.9%,                 Valid: loss=0.001, accuracy=83.9%\n",
            "Epoch  4,                 Train:loss=0.001, accuracy=85.8%,                 Valid: loss=0.001, accuracy=84.4%\n",
            "Epoch  5,                 Train:loss=0.001, accuracy=85.6%,                 Valid: loss=0.001, accuracy=84.1%\n",
            "validation accuracy = 84.400\n",
            "------------------------------------------------------------------\n",
            "Training model with a depth of 3 layers and a width of 25 units\n",
            "Teta params=[torch.Size([25, 785]), torch.Size([25, 26]), torch.Size([25, 26]), torch.Size([10, 26])]\n",
            "Epoch  0,                 Train:loss=0.005, accuracy=10.7%,                 Valid: loss=0.005, accuracy=9.9%\n",
            "Epoch  1,                 Train:loss=0.002, accuracy=72.9%,                 Valid: loss=0.002, accuracy=73.8%\n",
            "Epoch  2,                 Train:loss=0.001, accuracy=77.3%,                 Valid: loss=0.001, accuracy=77.8%\n",
            "Epoch  3,                 Train:loss=0.001, accuracy=78.0%,                 Valid: loss=0.001, accuracy=78.6%\n",
            "Epoch  4,                 Train:loss=0.001, accuracy=77.2%,                 Valid: loss=0.001, accuracy=77.3%\n",
            "Epoch  5,                 Train:loss=0.001, accuracy=79.0%,                 Valid: loss=0.001, accuracy=78.9%\n",
            "validation accuracy = 78.900\n",
            "------------------------------------------------------------------\n",
            "Training model with a depth of 3 layers and a width of 100 units\n",
            "Teta params=[torch.Size([100, 785]), torch.Size([100, 101]), torch.Size([100, 101]), torch.Size([10, 101])]\n",
            "Epoch  0,                 Train:loss=0.005, accuracy=10.5%,                 Valid: loss=0.005, accuracy=10.0%\n",
            "Epoch  1,                 Train:loss=0.001, accuracy=80.4%,                 Valid: loss=0.001, accuracy=80.5%\n",
            "Epoch  2,                 Train:loss=0.001, accuracy=82.4%,                 Valid: loss=0.001, accuracy=82.0%\n",
            "Epoch  3,                 Train:loss=0.001, accuracy=82.9%,                 Valid: loss=0.001, accuracy=82.5%\n",
            "Epoch  4,                 Train:loss=0.001, accuracy=81.8%,                 Valid: loss=0.001, accuracy=80.7%\n",
            "Epoch  5,                 Train:loss=0.001, accuracy=83.5%,                 Valid: loss=0.001, accuracy=82.8%\n",
            "validation accuracy = 82.850\n",
            "------------------------------------------------------------------\n",
            "Training model with a depth of 3 layers and a width of 300 units\n",
            "Teta params=[torch.Size([300, 785]), torch.Size([300, 301]), torch.Size([300, 301]), torch.Size([10, 301])]\n",
            "Epoch  0,                 Train:loss=0.005, accuracy=9.9%,                 Valid: loss=0.005, accuracy=10.2%\n",
            "Epoch  1,                 Train:loss=0.005, accuracy=10.0%,                 Valid: loss=0.005, accuracy=9.8%\n",
            "Epoch  2,                 Train:loss=0.005, accuracy=10.1%,                 Valid: loss=0.005, accuracy=9.4%\n",
            "Epoch  3,                 Train:loss=0.005, accuracy=10.0%,                 Valid: loss=0.005, accuracy=10.2%\n",
            "Epoch  4,                 Train:loss=0.005, accuracy=9.9%,                 Valid: loss=0.005, accuracy=11.0%\n",
            "Epoch  5,                 Train:loss=0.005, accuracy=10.0%,                 Valid: loss=0.005, accuracy=9.6%\n",
            "validation accuracy = 10.967\n",
            "------------------------------------------------------------------\n",
            "Training model with a depth of 3 layers and a width of 500 units\n",
            "Teta params=[torch.Size([500, 785]), torch.Size([500, 501]), torch.Size([500, 501]), torch.Size([10, 501])]\n",
            "Epoch  0,                 Train:loss=0.005, accuracy=11.0%,                 Valid: loss=0.005, accuracy=10.7%\n",
            "Epoch  1,                 Train:loss=0.005, accuracy=10.1%,                 Valid: loss=0.005, accuracy=10.3%\n",
            "Epoch  2,                 Train:loss=0.004, accuracy=13.3%,                 Valid: loss=0.004, accuracy=13.6%\n",
            "Epoch  3,                 Train:loss=0.005, accuracy=10.0%,                 Valid: loss=0.005, accuracy=10.0%\n",
            "Epoch  4,                 Train:loss=0.005, accuracy=10.0%,                 Valid: loss=0.005, accuracy=10.2%\n",
            "Epoch  5,                 Train:loss=0.005, accuracy=10.1%,                 Valid: loss=0.005, accuracy=9.3%\n",
            "validation accuracy = 13.600\n",
            "------------------------------------------------------------------\n",
            "Training model with a depth of 3 layers and a width of 1000 units\n",
            "Teta params=[torch.Size([1000, 785]), torch.Size([1000, 1001]), torch.Size([1000, 1001]), torch.Size([10, 1001])]\n",
            "Epoch  0,                 Train:loss=0.005, accuracy=13.6%,                 Valid: loss=0.005, accuracy=13.8%\n",
            "Epoch  1,                 Train:loss=0.001, accuracy=75.4%,                 Valid: loss=0.001, accuracy=75.3%\n",
            "Epoch  2,                 Train:loss=0.001, accuracy=78.9%,                 Valid: loss=0.001, accuracy=78.6%\n",
            "Epoch  3,                 Train:loss=0.001, accuracy=81.2%,                 Valid: loss=0.001, accuracy=80.8%\n",
            "Epoch  4,                 Train:loss=0.001, accuracy=82.0%,                 Valid: loss=0.001, accuracy=81.5%\n",
            "Epoch  5,                 Train:loss=0.001, accuracy=83.7%,                 Valid: loss=0.001, accuracy=82.9%\n",
            "validation accuracy = 82.933\n",
            "------------------------------------------------------------------\n",
            "Training model with a depth of 5 layers and a width of 25 units\n",
            "Teta params=[torch.Size([25, 785]), torch.Size([25, 26]), torch.Size([25, 26]), torch.Size([25, 26]), torch.Size([25, 26]), torch.Size([10, 26])]\n",
            "Epoch  0,                 Train:loss=0.005, accuracy=7.1%,                 Valid: loss=0.005, accuracy=7.2%\n",
            "Epoch  1,                 Train:loss=0.001, accuracy=73.5%,                 Valid: loss=0.001, accuracy=73.2%\n",
            "Epoch  2,                 Train:loss=0.001, accuracy=77.0%,                 Valid: loss=0.001, accuracy=76.8%\n",
            "Epoch  3,                 Train:loss=0.001, accuracy=79.7%,                 Valid: loss=0.001, accuracy=79.5%\n",
            "Epoch  4,                 Train:loss=0.001, accuracy=80.9%,                 Valid: loss=0.001, accuracy=80.6%\n",
            "Epoch  5,                 Train:loss=0.001, accuracy=81.4%,                 Valid: loss=0.001, accuracy=81.4%\n",
            "validation accuracy = 81.383\n",
            "------------------------------------------------------------------\n",
            "Training model with a depth of 5 layers and a width of 100 units\n",
            "Teta params=[torch.Size([100, 785]), torch.Size([100, 101]), torch.Size([100, 101]), torch.Size([100, 101]), torch.Size([100, 101]), torch.Size([10, 101])]\n",
            "Epoch  0,                 Train:loss=0.005, accuracy=10.2%,                 Valid: loss=0.005, accuracy=9.5%\n",
            "Epoch  1,                 Train:loss=0.003, accuracy=45.9%,                 Valid: loss=0.003, accuracy=45.2%\n",
            "Epoch  2,                 Train:loss=0.002, accuracy=50.3%,                 Valid: loss=0.002, accuracy=49.1%\n",
            "Epoch  3,                 Train:loss=0.002, accuracy=46.7%,                 Valid: loss=0.002, accuracy=47.2%\n",
            "Epoch  4,                 Train:loss=0.002, accuracy=55.9%,                 Valid: loss=0.002, accuracy=55.9%\n",
            "Epoch  5,                 Train:loss=0.002, accuracy=58.4%,                 Valid: loss=0.002, accuracy=57.8%\n",
            "validation accuracy = 57.750\n",
            "------------------------------------------------------------------\n",
            "Training model with a depth of 5 layers and a width of 300 units\n",
            "Teta params=[torch.Size([300, 785]), torch.Size([300, 301]), torch.Size([300, 301]), torch.Size([300, 301]), torch.Size([300, 301]), torch.Size([10, 301])]\n",
            "Epoch  0,                 Train:loss=0.005, accuracy=7.8%,                 Valid: loss=0.005, accuracy=8.4%\n",
            "Epoch  1,                 Train:loss=0.002, accuracy=66.3%,                 Valid: loss=0.002, accuracy=67.1%\n",
            "Epoch  2,                 Train:loss=0.001, accuracy=72.2%,                 Valid: loss=0.001, accuracy=72.4%\n",
            "Epoch  3,                 Train:loss=0.002, accuracy=60.5%,                 Valid: loss=0.002, accuracy=60.9%\n",
            "Epoch  4,                 Train:loss=0.005, accuracy=10.0%,                 Valid: loss=0.005, accuracy=9.7%\n",
            "Epoch  5,                 Train:loss=0.005, accuracy=10.0%,                 Valid: loss=0.005, accuracy=10.2%\n",
            "validation accuracy = 72.350\n",
            "------------------------------------------------------------------\n",
            "Training model with a depth of 5 layers and a width of 500 units\n",
            "Teta params=[torch.Size([500, 785]), torch.Size([500, 501]), torch.Size([500, 501]), torch.Size([500, 501]), torch.Size([500, 501]), torch.Size([10, 501])]\n",
            "Epoch  0,                 Train:loss=0.005, accuracy=6.6%,                 Valid: loss=0.005, accuracy=6.6%\n",
            "Epoch  1,                 Train:loss=0.005, accuracy=10.0%,                 Valid: loss=0.005, accuracy=9.6%\n",
            "Epoch  2,                 Train:loss=0.005, accuracy=10.1%,                 Valid: loss=0.005, accuracy=9.5%\n",
            "Epoch  3,                 Train:loss=0.005, accuracy=10.0%,                 Valid: loss=0.005, accuracy=10.3%\n",
            "Epoch  4,                 Train:loss=0.005, accuracy=10.0%,                 Valid: loss=0.005, accuracy=9.8%\n",
            "Epoch  5,                 Train:loss=0.005, accuracy=10.0%,                 Valid: loss=0.005, accuracy=10.4%\n",
            "validation accuracy = 10.400\n",
            "------------------------------------------------------------------\n",
            "Training model with a depth of 5 layers and a width of 1000 units\n",
            "Teta params=[torch.Size([1000, 785]), torch.Size([1000, 1001]), torch.Size([1000, 1001]), torch.Size([1000, 1001]), torch.Size([1000, 1001]), torch.Size([10, 1001])]\n",
            "Epoch  0,                 Train:loss=0.005, accuracy=7.8%,                 Valid: loss=0.005, accuracy=7.9%\n",
            "Epoch  1,                 Train:loss=0.006, accuracy=10.0%,                 Valid: loss=0.006, accuracy=10.2%\n",
            "Epoch  2,                 Train:loss=0.005, accuracy=10.0%,                 Valid: loss=0.005, accuracy=10.2%\n",
            "Epoch  3,                 Train:loss=0.005, accuracy=10.0%,                 Valid: loss=0.005, accuracy=10.2%\n",
            "Epoch  4,                 Train:loss=0.005, accuracy=10.1%,                 Valid: loss=0.005, accuracy=9.3%\n",
            "Epoch  5,                 Train:loss=0.005, accuracy=10.1%,                 Valid: loss=0.005, accuracy=9.3%\n",
            "validation accuracy = 10.250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Tableau pour la précision sur l'ensemble de validation**\n",
        "N.B. que les lignes correspondent aux nombre de couche et les colonnes correspondent au nombre de neurone dans chaque couche. Les valeurs ci-dessous sont donné comme exemples; remplacez-les par les valeurs que vous avez utilisées pour votre recherche d'hyperparamètres.\n",
        "\n",
        "depth\\width  | 25 | 100 | 300 | 500 | 1000\n",
        "-------------------|------------------|------------------|------------------|------------------|------------------|\n",
        "**1**   | -  | - | - | - | - |\n",
        "**3** | -  | - | - | - | - |\n",
        "**5**  | -  | - | - | - | - |"
      ],
      "metadata": {
        "id": "6LQ6q18CAUXP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adam: Analyse du meilleur modèle"
      ],
      "metadata": {
        "id": "df6Y9ziXAUXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ADAM\n",
        "# Montrez les résultats pour la meilleure configuration trouvez ci-dessus.\n",
        "depth = None    # TODO: Vous devez modifier cette valeur avec la meilleur que vous avez eu.\n",
        "width = None    # TODO: Vous devez modifier cette valeur avec la meilleur que vous avez eu.\n",
        "lr = 0.05           # Some value\n",
        "batch_size = 500   # Some value\n",
        "\n",
        "with torch.no_grad():\n",
        "  data_loader_train, data_loader_val, data_loader_test = get_fashion_mnist_dataloaders(val_percentage=0.1, batch_size=batch_size)\n",
        "\n",
        "  MLP_model = MLPModel(n_features=784, n_hidden_features=width, n_hidden_layers=depth, n_classes=10)\n",
        "  best_model, best_val_accuracy, logger = train(MLP_model,lr=lr, nb_epochs=5, sgd=False,\n",
        "                                                data_loader_train=data_loader_train, data_loader_val=data_loader_val)\n",
        "  logger.plot_loss_and_accuracy()\n",
        "  print(f\"Best validation accuracy = {best_val_accuracy*100:.3f}\")\n",
        "\n",
        "  accuracy_test, loss_test = accuracy_and_loss_whole_dataset(data_loader_test, best_model)\n",
        "print(\"Evaluation of the best training model over test set\")\n",
        "print(\"------\")\n",
        "print(f\"Loss : {loss_test:.3f}\")\n",
        "print(f\"Accuracy : {accuracy_test*100.:.3f}\")"
      ],
      "metadata": {
        "id": "uohnWTtoAUXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modèle avec autograd pyTorch"
      ],
      "metadata": {
        "id": "pbmmorf45iaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import copy\n",
        "\n",
        "\n",
        "def accuracy_and_loss_pytorch(data_loader, model):\n",
        "    cardinal = 0\n",
        "    # On utilise la fonction de pyTorch pour la Loss crossEntropy\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    total_loss = 0\n",
        "    n_accurate_preds = 0\n",
        "\n",
        "    model.eval()  # Mode évaluation\n",
        "    with torch.no_grad():\n",
        "        for x, y in data_loader:\n",
        "            x, y = reshape_input(x, y)\n",
        "            y_pred = model.forward(x)\n",
        "            y_classes = torch.argmax(y, dim=1)\n",
        "\n",
        "            # Calcul de la perte (CrossEntropy)\n",
        "            loss = loss_fn(y_pred, y_classes)\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "\n",
        "            # Calcul de la précision\n",
        "            _, (n_acc, n_samples) = accuracy(y, y_pred)\n",
        "            cardinal += n_samples\n",
        "            n_accurate_preds += n_acc\n",
        "\n",
        "    avg_loss = total_loss / cardinal\n",
        "    acc = n_accurate_preds / cardinal\n",
        "\n",
        "    return acc, avg_loss\n",
        "\n",
        "\n",
        "def train_pytorch(model, lr=0.1, nb_epochs=10, data_loader_train=None, data_loader_val=None):\n",
        "    best_model = None\n",
        "    best_val_accuracy = 0\n",
        "    logger = Logger()\n",
        "\n",
        "    # On choisit Adam comme optimiseur\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(nb_epochs+1):\n",
        "        # at epoch 0 evaluate random initial model\n",
        "        #   then for subsequent epochs, do optimize before evaluation.\n",
        "        if epoch > 0:\n",
        "            model.train()  # Mode entraînement\n",
        "            for x, y in data_loader_train:\n",
        "                x, y = reshape_input(x, y)\n",
        "\n",
        "                # Forward prop\n",
        "                optimizer.zero_grad()\n",
        "                y_pred = model(x)\n",
        "                y_classes = torch.argmax(y, dim=1)\n",
        "\n",
        "                # Calcul de la perte\n",
        "                loss = loss_fn(y_pred, y_classes)\n",
        "\n",
        "                # Backward prop\n",
        "                loss.backward()\n",
        "\n",
        "                # update\n",
        "                optimizer.step()\n",
        "\n",
        "        # Évaluation sur les ensembles d'entraînement et de validation\n",
        "        model.eval()  # Mode évaluation\n",
        "        accuracy_train, loss_train = accuracy_and_loss_pytorch(data_loader_train, model)\n",
        "        accuracy_val, loss_val = accuracy_and_loss_pytorch(data_loader_val, model)\n",
        "\n",
        "        if accuracy_val > best_val_accuracy:\n",
        "            best_val_accuracy = accuracy_val\n",
        "            best_model = copy.deepcopy(model)\n",
        "\n",
        "        logger.log(accuracy_train, loss_train, accuracy_val, loss_val)\n",
        "        print(f\"Epoch {epoch:2d}, \"\n",
        "              f\"Train: loss={loss_train:.3f}, accuracy={accuracy_train*100:.1f}%, \"\n",
        "              f\"Valid: loss={loss_val:.3f}, accuracy={accuracy_val*100:.1f}%\", flush=True)\n",
        "\n",
        "    return best_model, best_val_accuracy, logger\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class PyTorchNN(nn.Module):\n",
        "    def __init__(self, n_features, n_hidden_features, n_hidden_layers, n_classes):\n",
        "        super(PyTorchNN, self).__init__()\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "\n",
        "        # Première couche (input -> hidden)\n",
        "        self.layers.append(nn.Linear(n_features, n_hidden_features))\n",
        "\n",
        "        # Couches cachées (hidden -> hidden)\n",
        "        for _ in range(n_hidden_layers - 1):\n",
        "            self.layers.append(nn.Linear(n_hidden_features, n_hidden_features))\n",
        "\n",
        "        # Dernière couche (hidden -> output)\n",
        "        self.layers.append(nn.Linear(n_hidden_features, n_classes))\n",
        "\n",
        "\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # propagation dans les neurones cachés\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = self.activation(layer(x))\n",
        "\n",
        "        # dernière couche + softmax\n",
        "        x = self.layers[-1](x)\n",
        "        return nn.functional.softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "4k_aeq926UMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyse des Résultats"
      ],
      "metadata": {
        "id": "-wlDcZB-AUXP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Répondez içi..."
      ],
      "metadata": {
        "id": "M-Wi3CG3AUXP"
      }
    }
  ]
}